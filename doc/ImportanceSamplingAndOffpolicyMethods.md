# A Brief Review of Off-policy RL Methods from a Perspective of Sampling

> In this article, I want to give some of my opinions on the off-policy 
methods in RL. Basically, what I want to discuss is the importance
sampling and its application in off-policy methods.

## A Brief Recap on Off-policy Methods
Briefly, off-policy methods aim to evaluate/optimise a *target policy* $\pi$ based on the experience generated by *behavior policy* $b$.

Also, a requirement is that $\pi(a|s)>0$ implies $b(a|s)>0$.

## 1st Case: Importance Sampling in Off-policy Monte Carlo Methods
The simplest case of applying importance sampling should be in Off-policy Monte Carlo
(MC) methods. From my perspective, the core idea of MC methods is to take the returns of a state $s$ in all samples episodes as they are sampling in a distribution of returns followed by performing policy $\pi$ and then take the MC estimate as $v_\pi(s)$. Here, let's consider on-policy MC methods first and let's denote episodes as $\{e_1, e_2, \dots, e_i, \dots, e_n\}$ and denote the return for a state $S_t =s$ in $e_i$ as $G_t^i$ where $t$ is a time-step, then the MC estimate of $v_\pi(s)$ can be given as follow:

$$v_\pi(s) = \mathbb{E}[G_t|s] \\ = \sum_{i=1}^n p(G_t^i|e_i) \cdot G_t^i $$.

As we want to estimate the values under policy $\pi$, the $p(G_t^i)$ should follow the distribution of $\pi$. Then we can write $p(G_t^i)$ as follow:

$$p_\pi(G_t^i) = \prod_{k=t}^{T_i - 1} \pi(A_k|S_k)p(S_{k+1}|S_k, A_k)$$,

where $T_i$ is the length of $e_i$.



Ok, now, we have a big trouble: $p(S_{k+1}|S_k, A_k)$ is of course unknown. Otherwise, why do we need to do MC? We can use Dynamic Programming instead. However, fortunately, we don't need to worry about this. Let's replace $\pi$ with another policy $b$, we can write $p_b(G_t^i)$ as  follow:

$$p_\pi(G_t^i) = \prod_{k=t}^{T_i - 1} \pi(A_k|S_k)p(S_{k+1}|S_k, A_k)$$


## 2nd Case: Why there isn't Importance Sampling in *one-step* Off-policy Temporal Difference Methods

## 3rd Case: Importance Sampling in *n-step* Off-policy TD Methods

## 4th Case: *n-step* Off-policy TD methods without Importance Sampling

## 5th Case: A Unified Point of View