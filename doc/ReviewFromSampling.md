# A Brief Review of Off-policy RL Methods from a Perspective of Sampling

> In this article, I want to give some of my opinions on the off-policy 
methods in RL. Basically, what I want to discuss is the importance
sampling and its application in off-policy methods.

## A Brief Recap on Off-policy Methods
Briefly, off-policy methods aim to evaluate/optimise a *target policy* 
<img src="https://latex.codecogs.com/svg.latex?\Large&space;\pi" title="\pi" />
based on the experience generated by *behavior policy*
<img src="https://latex.codecogs.com/svg.latex?\Large&space;b" title="b" />.

Also, a requirement is that 
<img src="https://latex.codecogs.com/svg.latex?\Large&space;\pi (a|s)>0" title="\pi (a|s) > 0" />
implies
<img src="https://latex.codecogs.com/svg.latex?\Large&space;b(a|s)>0" title="b(a|s)>0" />.

## 1st Case: Importance Sampling in Monte Carlo Methods

## 2nd Case: Why there isn't Importance Sampling in *one-step* Off-policy Temporal Difference Methods

## 3rd Case: Importance Sampling in *n-step* Off-policy TD Methods

## 4th Case: *n-step* Off-policy TD methods without Importance Sampling

## 5th Case: A Unified Point of View