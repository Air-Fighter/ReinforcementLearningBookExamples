# A Brief Review of Off-policy RL Methods from a Perspective of Sampling

> In this article, I want to give some of my opinions on the off-policy 
methods in RL. Basically, what I want to discuss is the importance
sampling and its application in off-policy methods.

## A Brief Recap on Off-policy Methods
Briefly, off-policy methods aim to evaluate/optimise a *target policy* $\pi$ based on the experience generated by *behavior policy* $b$.

Also, a requirement is that $\pi(a|s)>0$ implies $b(a|s)>0$.

## 1st Case: Importance Sampling in Off-policy Monte Carlo Methods
The simplest case of applying importance sampling should be in Off-policy Monte Carlo
(MC) methods. But, let's first consider the on-policy MC methods. From my perspective, the core idea of on-policy MC methods is to take the returns of a state $s$ in all samples episodes as they are sampling in a distribution of returns and then take the MC estimate as $v(s)$. Here, let's denote episodes as $\{e_1, e_2, \dots, e_i, \dots, e_n\}$ and denote the return for a state $s$ in $e_i$ as $G_t^i$ where $t$ is the time-step such that $S_t = s$, then the MC estimate of $v(s)$ can be given as follow:

$$v(s) = \mathbb{E}[G_t|s] \\ = \sum_{i=1}^n p(e_i) \cdot G_t^i $$.

As we want to estimate the values under policy $\pi$, the $p(e_i)$ should follow the distribution of $\pi$. Then we can write $p(e_i)$ as follow:

$$p_\pi(e_i) = \prod_{k=1}{T_i - 1} \pi(A_k|S_k)p(S_{k+1}|S_k, A_k)$$,

where $T_i$ is the length of $e_i$.



## 2nd Case: Why there isn't Importance Sampling in *one-step* Off-policy Temporal Difference Methods

## 3rd Case: Importance Sampling in *n-step* Off-policy TD Methods

## 4th Case: *n-step* Off-policy TD methods without Importance Sampling

## 5th Case: A Unified Point of View